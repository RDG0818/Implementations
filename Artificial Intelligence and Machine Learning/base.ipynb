{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aba594eb",
   "metadata": {},
   "source": [
    "# Analysis of [Insert Algorithm]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775ace3a",
   "metadata": {},
   "source": [
    "## Theory\n",
    "\n",
    "### Motivation\n",
    "\n",
    "Include the gap/current limitation. Include an example of a real-world problem.\n",
    "\n",
    "**Example:** \n",
    "\n",
    "While Linear Regression is effective for predicting continuous values (e.g., house prices), it is ill-suited for classification tasks. For a binary problem (e.g., predicting Yes/No), Linear Regression's output is unbounded (−∞,∞), making it difficult to interpret as a class prediction. A model was needed that could directly estimate the probability of a binary outcome, ensuring its output is constrained between 0 and 1.\n",
    "\n",
    "Consider a common medical diagnostic task: predicting whether a patient has a certain disease based on various clinical measurements (e.g., age, blood pressure, cholesterol level). The outcome is binary (Disease / No Disease). We need a model that can take the patient's data and output the probability that they have the disease, providing a simple and interpretable result for doctors to act upon.\n",
    "\n",
    "### Intuition\n",
    "\n",
    "Core idea/one-sentence summary. Also include a conceptual walkthrough.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Logistic Regression models the probability of a binary outcome by passing a linear combination of features through a \"squashing\" function, known as the logistic (or sigmoid) function.\n",
    "\n",
    "1. Calculate a \"score\" by taking a weighted sum of the input features. This creates a decision boundary in the feature space.\n",
    "2. A high positive score suggest the data point belongs to a positive class, while a large negative score suggest it belongs to the negative class.\n",
    "3. Use the sigmoid function, $\\sigma(z)=1/(1+e^{-z})$, to map this unbounded score into the range $[0, 1]$. This \"squashed\" value can now be interpreted as the probability of the positive class.\n",
    "4. Use a threshold (ex. 0.5) to determine if the model should predict the data as the positive or negative class.\n",
    "\n",
    "### Formal Definition\n",
    "\n",
    "Includes hypothesis space, loss function, learning principle, and generalization bounds. Define all variables used in the definition.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Let the input features for a single data point be a vector $x \\in \\mathbb{R}^D$, and the binary label be $y \\in {0, 1}$. The model is parameterized by a weight vector $\\theta \\in \\mathbb{R}^D$.\n",
    "\n",
    "- Hypothesis Space ($H$): The set of all possible functions the model can learn. For logistic regression, these are functions that compute a probability by applying the sigmoid function to a linear combination of the input features. \n",
    "\n",
    "$$\n",
    "H = \\left\\{h_{\\theta}(x) \\;\\middle|\\; h_{\\theta}(x) = \\sigma(\\theta^Tx) = \\frac{1}{1+e^{-\\theta^Tx}}, \\text{for } \\theta \\in \\mathbb{R}^D \\right\\}\n",
    "$$\n",
    "\n",
    "- Loss Function (L): To measure the error for a single prediction, we use the binary Cross-Entropy or Log Loss. For a true label $y$ and a predicted probability $\\hat{y} = h_\\theta(x)$, the loss is:\n",
    "\n",
    "$$\n",
    "L(y, \\hat(y)) = -\\left[y\\log(\\hat{y})+(1-y)\\log(1-\\hat(y))\\right]\n",
    "$$\n",
    "\n",
    "- Learning Principle: The goal is to find the parameters $\\hat{\\theta}$ that minimize the average log loss over the entire training set $D$ (Empirical Risk Minimization). The objective function, $J(\\theta)$, is:\n",
    "\n",
    "$$\n",
    "\\hat{\\theta} = \\arg\\min_{\\theta} J(\\theta) = \\arg\\min_{\\theta}\\frac{1}{N}\\sum_{i=1}^{N}L(y_i, h_\\theta(x_i))\n",
    "$$\n",
    "\n",
    "- Generalization Bounds: For linear classifiers like Logistic Regression, theoretical generalization bounds (which connect training error to test error) are often expressed in terms of the margin of separation. These bounds, derived from theories like VC-dimension or Rademacher complexity, suggest that if the model finds a decision boundary that separates the training data with a large margin, it is likely to generalize well to unseen data.\n",
    "\n",
    "### Pseudocode\n",
    "\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "\\hline\n",
    "\\textbf{Algorithm 1: [Algorithm Name] - Instance} \\\\\n",
    "\\hline\n",
    "\\textbf{Require: } \\text{Training set } D = \\{(x_1, y_1), ..., (x_N, y_N)\\}, \\text{ integer } k, \\text{ new point } x_q \\\\\n",
    "\\textbf{Ensure: } \\text{Predicted label } y_q \\\\\n",
    "\\\\\n",
    "\\text{1: Initialize a list of distances } \\textit{dist\\_list} \\\\\n",
    "\\text{2: \\textbf{for} } i = 1 \\text{ to } N \\textbf{ do} \\\\\n",
    "\\text{3: \\quad Calculate distance } d(x_q, x_i) \\\\\n",
    "\\text{4: \\quad Append } (d(x_q, x_i), y_i) \\text{ to } \\textit{dist\\_list} \\\\\n",
    "\\text{5: \\textbf{end for}} \\\\\n",
    "\\text{6: Sort } \\textit{dist\\_list} \\text{ by distance in ascending order} \\\\\n",
    "\\text{7: Select the first } k \\text{ elements: } D_k \\subseteq \\textit{dist\\_list} \\\\\n",
    "\\text{8: \\textbf{return} } y_q = \\text{argmax}_{y_c} \\sum_{(d, y) \\in D_k} I(y=y_c) \\quad \\triangleright \\text{Return the majority class in } D_k \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "\\hline\n",
    "\\textbf{Algorithm 1: [Algorithm Name] - Training} \\\\\n",
    "\\hline\n",
    "\\textbf{Require: } \\text{Training set } D = \\{(x_1, y_1), \\dots, (x_N, y_N)\\} \\\\\n",
    "\\textbf{Require: } \\text{Hyperparameters (e.g., learning rate } \\alpha, \\text{ regularization } \\lambda) \\\\\n",
    "\\textbf{Ensure: } \\text{Trained model parameters } \\theta \\\\\n",
    "\\\\\n",
    "\\text{1: Initialize parameters } \\theta \\text{ (e.g., with zeros or small random values)} \\\\\n",
    "\\text{2: \\textbf{repeat}} \\\\\n",
    "\\text{3: \\quad Sample a mini-batch } D_b \\subseteq D \\quad \\triangleright \\text{For stochastic/mini-batch methods} \\\\\n",
    "\\text{4: \\quad Compute gradient of the loss on the batch: } \\nabla J(\\theta) = \\frac{1}{|D_b|} \\sum_{(x_i, y_i) \\in D_b} \\nabla_{\\theta} L(y_i, f(x_i; \\theta)) \\\\\n",
    "\\text{5: \\quad Update parameters using the gradient: } \\theta \\leftarrow \\theta - \\alpha \\nabla J(\\theta) \\\\\n",
    "\\text{6: \\textbf{until} a stopping condition is met} \\quad \\triangleright \\text{e.g., max iterations or convergence} \\\\\n",
    "\\text{7: \\textbf{return} } \\theta \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "\\hline\n",
    "\\textbf{Algorithm 2: [Algorithm Name] - Prediction} \\\\\n",
    "\\hline\n",
    "\\textbf{Require: } \\text{Trained model parameters } \\theta \\\\\n",
    "\\textbf{Require: } \\text{A new data point } x_q \\\\\n",
    "\\textbf{Ensure: } \\text{A prediction for the new point, } \\hat{y}_q \\\\\n",
    "\\\\\n",
    "\\text{1: Compute the raw model output (score or logits): } s_q = f(x_q; \\theta) \\\\\n",
    "\\text{2: Post-process the output to get the final prediction } \\hat{y}_q \\\\\n",
    "\\text{3: \\quad \\textbf{if} problem is classification \\textbf{then}} \\\\\n",
    "\\text{4: \\quad \\quad } \\hat{y}_q \\leftarrow \\text{argmax}_{c} (s_q)_c \\quad \\triangleright \\text{Return class with the highest score} \\\\\n",
    "\\text{5: \\quad \\textbf{else if} problem is regression \\textbf{then}} \\\\\n",
    "\\text{6: \\quad \\quad } \\hat{y}_q \\leftarrow s_q \\quad \\triangleright \\text{Return the score directly} \\\\\n",
    "\\text{7: \\quad \\textbf{end if}} \\\\\n",
    "\\text{8: \\textbf{return} } \\hat{y}_q \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "### Derivations and Convergence\n",
    "\n",
    "Include properties of the objective function and derivation of the loss function from the objective function. State established convergence guarantees and statistical consistency (does the model converge as data grows infinitely large?) when applicable.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "- Properties of the Objective Function: The objective function for logistic regression is the negative log-likelihood, which is a convex function. This guarantees that any local minimum is also the global minimum.\n",
    "- Derivation of the Gradient: Let $\\hat{y} = \\sigma(\\theta^Tx)$. The derivative of the sigmoid is $\\sigma'(z)=\\sigma(z)(1-\\sigma(z))$. Using the chain rule on $L=-\\left[y\\log(\\hat{y})+(1-y)\\log(1-\\hat{h})\\right]$:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta L = \\frac{\\partial L}{\\partial \\hat{y}} \\frac{\\partial \\hat{y}}{\\partial (\\theta^T x)} \\frac{\\partial (\\theta^Tx)}{\\partial \\theta} = (-\\frac{y}{\\hat{y}}+\\frac{1-y}{1-\\hat{y}}) \\cdot (\\hat{y}(1-\\hat(y))) \\cdot x = (-y(1-\\hat{y}) + (1-y)\\hat{y})\\cdot x = (-y + y\\hat{y} = \\hat{y} - y\\hat{y}) \\cdot x = (\\hat{y} - y)x\n",
    "$$\n",
    "\n",
    "- Convergence Guarantees: Because the objective function is convex, standard gradient descent with a sufficiently small learning rate $\\alpha$ is guaranteed to converge to the unique global minimum of the objective function.\n",
    "- Statistical Consistency: Logistic regression is a consisten estimator. As the number of training samples $N \\rightarrow \\inf$, the estimated parameters $\\hat{\\theta}_N$ converge in probability to the true parameters $\\theta^*$ that define the underlying data distribution.\n",
    "\n",
    "### Complexity Analysis\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Let $N$ be the number of training samples, $D$ be the number of features, and $I$ be the number of training iterations.\n",
    "\n",
    "| Complexity Type | Training Time | Prediction Time |\n",
    "| :--- | :--- | :--- |\n",
    "| **Time** | $O(I \\cdot N \\cdot D)$ | $O(D)$ | \n",
    "| **Space** | $O(N \\cdot D)$ | $O(N \\cdot D)$ | \n",
    "| **Sample** | Dependent on data density | | \n",
    "\n",
    "Justification for each complexity type is also necessary.\n",
    "\n",
    "- Time: Training requires $I$ passes over the data (epochs). Each pass involves computing a dot product for $N$ samples, taking $O(N \\cdot D) time. Prediction only requires a single dot product for one point, taking $O(D)$ time.\n",
    "- Space: During training, the entire dataset must typically be held in memory. The final trained model, however, only requires storing the parameter vector $\\theta$, which is of size $D$. \n",
    "\n",
    "### Relation to Other Algorithms\n",
    "\n",
    "Include a comparison to at least one other algorithm. Is this algorithm a special case of another, more general one?\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Comparison to SVMs: Both are linear classifiers, with the difference in their loss functions. Logistic Regression uses Log Loss, which provids a probabilistic output. An SVM with a linear kernel uses Hinge Loss, whiche aims to maximize the margin between the classes and is not inherently probabilistic. \n",
    "\n",
    "Logistic Regression can be viewed as a General Linear Model. It uses the same linear predictor, $\\theta^{T}x$, as linear regression, but then applies a non-linear \"link function\" (the logit function) to model probabilites instead of continuous values.\n",
    "\n",
    "### Assumptions and Limitations\n",
    "\n",
    "- Assumes that the log-odds of the outcome are a linear function of the input features. The model cannot capture complex, non-linear relationships between features and the outcome without manual feature engineering.\n",
    "- Assumes the training examples are independent of each other.\n",
    "- If features are highly correlated, the model's parameter estimates can become unstable and difficult to interpret. \n",
    "\n",
    "### Pros and Cons\n",
    "\n",
    "| Pros | Cons |\n",
    "| :--- | :--- |\n",
    "| Highly inerpretable; coefficients can be seen as feature importances. | The assumption of linearity limits its use for complex non-linear problems. |\n",
    "| Outputs well-calibrated probabilities, not just class labels | Can be prone to underfitting if the decision boundary is not linear. |\n",
    "| Computationally efficient and fast to train and predict | Performance can be affected by strong multicollinearity among features. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0575b7",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2e071ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World\n"
     ]
    }
   ],
   "source": [
    "# Insert Code here\n",
    "print(\"Hello World\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79720535",
   "metadata": {},
   "source": [
    "## Experimental Analysis\n",
    "\n",
    "### Performance Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db56da2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare accuracy, precision, recall, f1-score, mse, silhouette score\n",
    "# compare run-times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be28c54",
   "metadata": {},
   "source": [
    "### Parameter Sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df142a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some code to plot this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec203118",
   "metadata": {},
   "source": [
    "### Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432501ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some code to plot like decision boundaries"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_Stuff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
